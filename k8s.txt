
dockerfile：
    VOLUME /data    
        #容器里面创建一个挂载点，如果没有在启动的时候指定挂载 -v(把本地目录挂载进去)，也会在docker目录中
        自动创建一个  /var/lib/docker/volumnes/ 下创建一个目录挂载到容器里面

制作小镜像：
    不要用centos
    alpine  busybox

    多阶段制作：
        正常一个带有go环境的slim包，打包下来200多。
        思路：
            先用go环境的slim包打成二进制文件，在用到 aipine系统中
        
            FROM golang:1.14.4-alpine as go-builder-stage
            WORKDIR /opt
            COPY main.go /opt
            RUN go build /opt/main.go
            CMD "./main"

            FROM alpine:3.8
            COPY --from=go-builder-stage /opt/main /
            CMD "main"



scratch镜像：






k8s会在 template里面的东西修改的时候去创建新的 replicationSet

一般发版使用set命令： kubectl set image deploy nginx nginx=nginx:1.15.4 --record

 回滚：
    查看历史记录：
        kubectl rollout history deploy nginx
    回滚到上一个版本：
        kubectl rollout undo deploy nginx 
    回滚到指定版本：
        查看指定版本的详细信息
        kubectl rollout history deploy nginx --revision=5
        回滚到指定的版本
        kubectl rollout  undo deploy nginx --to-revision=5

扩容：
    kubectl scale --replicas=3 -f foo.yaml
    kubectl scale --replicas=3 deploy nginx
    kubectl scale --replicas=3 sts web

HPA:
    Horizontal pod Autoscaler: Pod的水平自动伸缩器
    观察pod的cpu，内存使用率自动扩展或缩容pod的数量
    不适用于无法缩放的对象，比如 DaemonSet
    只支持cpu，内存


deployment暂停：
    多次修改，一次更新 
    kubectl rollout pause deployment nginx



StatefulSet：
    更新策略：
    updateStrategy
        rollingUpdate:  # type为rollingUpdate时存在，onDelete时直接删除
            partition: 0    #启动5个pod，设置2，为保留小于2的
        type: OnDelete # 删除pod后才会更新   rollingUpdate 滚动更新

    级联删除和非级联删除：
        级联删除: 删除sts时同时删除pod（默认）
        非级联删除：删除sts时不删除pod
            kubectl delete sts web --cascade=false , 此时pod为孤儿pod，删除后不会再重建


DaemonSet:
    缩写ds，在所有节点或者是匹配的节点上部署一个Pod
    更新建议用 OnDelete

打标签：
    kubectl label node k8s-node01 k8s-node02 ds=true 




k8s探针：
    LivenessProbe:
        判断容器状态是否存活或者健康，不健康则kubelet杀掉该容器，并根据容器的重启策略做相应的处理。
    ReadinessProbe:
        判断容器是否启动完成，可以接收请求。
        探测失败，则pod的状态被修改，Endpoint Controller将从Service的Endpoint中删除包含该容器所在的pod的Endpoint
    StartupProbe:
        指示容器中的应用是否已经启动，如果提供了startup probe，则禁用其他所有其他探针，直到它成功为止。
        启动探针失败，kubelet杀掉该容器，并根据容器的重启策略做相应的处理。


如何组合使用？
1. 启动探针 startupProbe。 
startupProbe:
    httpGet:
        path: /doc.html
        port: 40017
    initialDelaySeconds: 10
    failureThreshold: 10
    periodSeconds: 5

    容器启动10秒后，startupProbe首先检测，应用程序最多有50秒(10次 * 5s = 50s)完成启动。
    一旦startupProbe成功一次，livenessProbe将接管，以对后续运行过程中容器死锁提供快速响应。
    如果startupProbe从未成功，则容器将在50s后被杀死

2. 就绪探针 ReadinessProbe
    验证容器内的服务可以正常提供服务
readinessProbe:
    httpGet:
        path: /doc.html
        port: 40017
    initialDelaySeconds: 10
    failureThreshold: 3
    periodSeconds: 5 

    ReadinessProbe会确保服务在你指定的探测命令执行成功的情况下才开始接受请求。

3. 存活探针 LivenessProbe 来对服务的长时间健康检测
livenessProbe:
    httpGet:
        path: /doc.html
        port: 40017
    failureThreshold: 2
    periodSeconds: 5 

    livenessProbe的设计是为了在pod启动成功后进行健康探测



k8s钩子：
    PostStart： 创建容器之后立即执行。不能保证钩子会在容器入口点之前执行。
    PreStop: 容器终止之前是否立即调用此钩子

    动作：
        exec - 执行一个特定的命令，在容器的cgroups和名称空间中

        lifecycle:
            postStart：
                exec:
                    command: ["/bin/sh", "-c", "echo 1"]
            preStop：
                exec:
                    command: ["/bin/sh", "-c", "echo 1"]


    containers:
        - name: lifecycle
          image: nginx
          lifecycle:
            preStop：
                httpGet:
                    path: /preStop
                    port: 8000
                    scheme: HTTP    






service：
    类型：
        ClusterIP: 集群内部使用
        ExternalName： 通过返回定义的CNAME别名
        NodePort： 在所有安装了 kube-proxy的节点上打开一个端口，此端口可以代理至后端Pod。
            然后集群外部可以使用节点的ip地址合NodePort的端口号访问到集群pod的服务



    可以反代k8s集群外部服务。
        让k8s去访问service服务，而不是集群外的ip地址或者域名
        例如，在部分程序迁移至k8s集群时，想访问外面的mq，redis，mysql等中间件时
        1. 手动创建service
        2. 手动创建endpoint
        #service.yaml

            apiVersion: v1
            kind: Service
            metadata:
              labels: 
                app: nginx-svc-external
              name: nginx-svc-external
            sepc:
              ports:
                - name: http
                  port: 80
                  protocol: TCP
                  targetPort: 80
              sessionAffinity: None
              type: ClusterIP

        #endpoint.yaml

            apiVersion: v1
            kind: Endpoint
            metadata:
              labels:
                app: nginx-svc-external
              name: nginx-svc-external
              namespace: default
            subsets:
              - addresses:
                - ip: 10.244.195.32
                ports:
                - name: http
                  port: 80
                  protocol: TCP


反代k8s集群外的域名。

1. 手动创建service
   1.1 从Pod中访问外部服务：
       最简单正确的方法是创建 ExternalName service    
#nginx-externalname.yaml

apiVersion: v1
kind: Service
metadata:
  labels:
    app: nginx-externalname
    name: nginx-externalname
sepc:
  type: ExternalName
  externalName: www.baidu.com



  1.2 可以用于访问其他名称空间的服务:
       namespace-a中的test-service-1，可以访问到namespace-b中的test-service-2

apiVersion: v1
kind: Service
metadata:
  name: test-service-1
  namespace: namespace-a
spec:
  type: ExternalName
  externalName: test-service-2.namespace-b.svc.cluster.local
  ports:
  - port: 80







taint & toleration：
    taint: 
        在一类服务器上打上污点，让不能容忍这个污点的pod不能部署在打了污点的服务器上。定义在node上。
        Node节点有多个Taint，每个Taint都需要容忍才能部署上去。

        允许master节点部署pod
        kubectl taint nodes --all node-role.kubernetes.io/control-plane- node-role.kubernetes.io/master-
        
        如果不允许调度
        kubectl taint nodes k8s-master node-role.kubernetes.io/master=:NoSchedule
        污点可选参数
        NoSchedule: 一定不能被调度
        PreferNoSchedule: 尽量不要调度
        NoExecute: 不仅不会调度, 还会驱逐Node上已有的Pod

        Taint的效果是NoSchedule。这意味着除非pod明确声明可以容忍这个Taint，否则就不会被调度到 k8s-master 上

        删除污点：
            kubectl taint nodes k8s-master master:NoSchedule-  #只删除 master 下的 NoSchedule 污点
            kubectl taint node k8s-master master- #删除master整个污点

    Toleration:
        定义在pod上。
        让pod能够（注意，只是能够，而非必须）运行在标注了Taint的node上。 
        operator 的默认值是 Equal;
        如果 operator 是 Exists （此时容忍度不能指定 value）
        如果 operator 是 Equal ，则它们的 value 应该相等
        如果 operator 不指定，则默认为Equal
        如果 effect 为空，则可以与所有键名 key1 的效果相匹配.
        -----------> (一对key和value，可以对应对个污点，所以 effect是指定具体是哪个污点，不指定则匹配所有
            例如：master=true 可以对应多个
            kubectl taint node k8s-master master=true:NoExecute
            kubectl taint node k8s-master master=true:NoSchedule
        
        tolerations:
        - key: "key1"
          operator: "Equal"
          value: "value1"
          effect: "NoSchedule"

        tolerations:
        - key: "key1"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 3600  #如果 Pod 存在一个 effect 值为NoExecute的容忍度指定了可选属性tolerationSeconds的值，则表示在给节点添加了上述污点之后， Pod 还能继续在节点上运行的时间
        



#打污点
[root@k8s-master ~]# kubectl taint node k8s-master master=true:NoSchedule
node/k8s-master tainted

#跑一个nginx，不设置容忍，结果如下：
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
Warning  FailedScheduling  26s   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {master: true}.
preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.

# 1 node(s) had untolerated taint {master: true}。 一个节点有不能容忍的污点

#给pod中加入污点容忍：

apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: ingress-nginx-test
  name: nginx-deployment-taint
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:alpine
          ports:
            - containerPort: 80
      tolerations:  #可以容忍节点上有污点，master=true:NoSchedule 的
      - key: "master"
        value: "true"
        effect: "NoSchedule"

      # 这个应该是新写法？
      tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 300






## Affinity 亲和性
    nodeSelector
        这个是直接去选择要部署的node的标签
        定向，精准调度策略
        通过node的label来匹配

        查看node的label
            kubectl get nodes --show-labels
        给node增加label
            kubectl label nodes node1 app=test
        node删除label，名字加一个减号
            kubectl label nodes k8s app-
        修改label
            kubectl label nodes k8s label/env=test1 --overwrite
        在 pod 的yaml文件中spec字段添加:
            这样这个pod就会部署在label是 app=test的 node节点上

        apiVersion: v1
        kind: Pod
        metadata:
          name: nginx
          labels:
            env: test
        spec:
          containers:
          - name: nginx
            image: nginx
            imagePullPolicy: IfNotPresent
          nodeSelector:
            app: test



    亲和性种类：
        nodeAffinity(节点亲和性)
        podAffinity(Pod亲和性)

        同时包含NodeSelector和NodeAffinity：必需同时满足 ########

    控制pod可以部署在哪些node上，不可以在那些node上
    node的亲和性和 NodeSelector 类似，增强了两点优势：

        1.可以存在软限制，即有优先级策略，可以存在退而求其次的情况
        2.可以延伸出Pod质检的亲和互斥关系，依据节点正在运行的其他Pod的标签进行限制

    调度策略：
        软策略与硬策略
        软策略：满足条件最好，不满足也可以。preferredDuringSchedulingIgnoredDuringExecution
        硬策略：必须满足，不然就不干了。requiredDuringSchedulingIgnoredDuringExecution



    --------------------- 举例 ----------------------------
    #给node设置一个label
        #kubectl label node k8s-master node=master

    #kubectl get nodes --show-labels
    NAME         STATUS   ROLES           AGE   VERSION   LABELS
    k8s-master   Ready    control-plane   9d    v1.24.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=true,
    kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,
    node.kubernetes.io/exclude-from-external-load-balancers=,node=master

    #给deployment设置亲和力node不为master

    apiVersion: apps/v1
    kind: Deployment
    metadata:
      namespace: ingress-nginx-test
      name: nginx-deployment-affinity
    spec:
      selector:
        matchLabels:
          app: nginx
      replicas: 2
      template:
        metadata:
          labels:
            app: nginx
        spec:
          containers:
            - name: nginx
              image: nginx:alpine
              ports:
                - containerPort: 80
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                  - key: node
                    operator: NotIn
                    values:
                    - master

    则部署的结果：
         不符合node的亲和力
         Warning  FailedScheduling  2m24s  default-scheduler
         0/1 nodes are available: 1 node(s) didn't match Pod's node affinity/selector. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling

    修改：
        requiredDuringSchedulingIgnoredDuringExecution  -->  preferredDuringSchedulingIgnoredDuringExecution

    # 下面同样是 node not in master，还是调度到了 master上，这个就是软调度

    apiVersion: apps/v1
    kind: Deployment
    metadata:
      namespace: ingress-nginx-test
      name: nginx-deployment-affinity
    spec:
      selector:
        matchLabels:
          app: nginx
      replicas: 2
      template:
        metadata:
          labels:
            app: nginx
        spec:
          containers:
            - name: nginx
              image: nginx:alpine
              ports:
                - containerPort: 80
          affinity:
            nodeAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 1
                preference:
                  matchExpressions:
                  - key: node
                    operator: NotIn
                    values:
                    - master

    # requiredDuringSchedulingIgnoredDuringExecution 和  preferredDuringSchedulingIgnoredDuringExecution 可以同时存在

    注：
    	如果指定多个nodeSelectorTerms，满足其中一个即可
    	如果指定多个matchExpressions，必须满足所有
    	同时包含NodeSelector和NodeAffinity：必需同时满足

    操作符operator：
        In：label 的值在某个列表中
        NotIn：label 的值不在某个列表中
        Gt：label 的值大于某个值
        Lt：label 的值小于某个值
        Exists：某个 label 存在
        DoesNotExist：某个 label 不存在




    以下是 pod的亲和性：
        podAffinity
        根据在节点上正在运行的Pod标签进行判断和调度
        是倾向于与某些满足条件的pod部署在一起。可以跨namespace的。也就是说与之匹配的pod在哪个node，则自己就部署在哪个节点上

        topologykey: 调度范围
            不同的key不同的value是属于不同的拓扑域
            如果加了拓扑域 topologyKey，则要满足拓扑域，满足亲和力的pod或者node要满足相同的拓扑域
            例如：
                在用反亲和力时，如果加了拓扑域，则可以实现应用不会部署在相同的拓扑域中、（如果拓扑域是不同地域，这实现了跨地域多活）

    apiVersion: v1
    kind: Pod
    metadata:
      name: with-pod-affinity
    spec:
      affinity:
        podAffinity:   #pod的亲和。当区域目标节点上且至少运行了一个，键security且值为S1的标签的pod，才可以将该pod调度节点上
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: security
                operator: In
                values:
                - S1
            topologyKey: failure-domain.beta.kubernetes.io/zone   #内置节点标签

        podAntiAffinity:  #pod的反亲和。和上面相反，如果区域目标节上具有键"security"和值"S2"的标签的pod处于相同的区域，pod不能被调度到该节点上
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: security
                  operator: In
                  values:
                  - S2
              topologyKey: failure-domain.beta.kubernetes.io/zone
      containers:
      - name: with-pod-affinity
        image: k8s.gcr.io/pause:2.0

    内置节点：
        # kubectl get node -owide --show-labels 能看到部分node默认的标签

        kubernetes.io/hostname   主机范围/node范围
        failure-domain.beta.kubernetes.io/zone  区域范围
        failure-domain.beta.kubernetes.io/region
        beta.kubernetes.io/instance-type
        kubernetes.io/os  系统范围
        kubernetes.io/arch  发行版

    topologyKey说明
        topologyKey 可以是任何合法的标签键。然而，出于性能和安全原因，topologyKey 受到一些限制

    对于亲和与 requiredDuringSchedulingIgnoredDuringExecution 要求的 pod
    反亲和，topologyKey 不允许为空。

    对于 requiredDuringSchedulingIgnoredDuringExecution 要求的 pod 反亲和，准入控制器
    LimitPodHardAntiAffinityTopology 被引入来限制 topologyKey 不为
    kubernetes.io/hostname。如果你想使它可用于自定义拓扑结构，你必须修改准入控制器或者禁用它。

    对于 preferredDuringSchedulingIgnoredDuringExecution 要求的 pod 反亲和，空的
    topologyKey 被解释为“所有拓扑结构”（这里的“所有拓扑结构”限制为
    kubernetes.io/hostname，failure-domain.beta.kubernetes.io/zone 和
    failure-domain.beta.kubernetes.io/region 的组合）。

    除上述情况外，topologyKey 可以是任何合法的标签键。


    apiVersion: apps/v1beta1
    kind: Deployment
    metadata:
      name: affinity
      labels:
        app: affinity
    spec:
      replicas: 2
      revisionHistoryLimit: 15
      template:
        metadata:
          labels:
            app: affinity
        spec:
          containers:
          - name: nginx
            image: nginx
            ports:
            - containerPort: 80
              name: nginxweb
          affinity:
            podAntiAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:  # 硬策略
              - labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - busybox-pod
                topologyKey: kubernetes.io/hostname

    解释：如果一个节点上面有一个app=busybox-pod这样的 pod 的话，那么我们的 pod 就别调度到这个节点上面来


    举例：

    在该Pod中，硬限制与security=s1的Pod在同一个zone中
    于此同时互斥调度配置为不与app=nginx的Pod在同一Node上





临时容器：
    在原有pod上，添加一个临时的Container。这个container包含需要排查的工具，类似 netstat , ps, top, jmap, jstat 等
    建议直接安装：1.25系统
    EphemeralContainers	true	Beta	1.23	1.24
    EphemeralContainers	true	GA	1.25	-

    1.24配置
        vim /var/lib/kubelet/config.yaml
        末尾添加：
        featureGates:
          EphemeralContainers: true

        systemctl daemon-reload
        systemctl restart kubectl
        如果是二进制安装：则要改所有的组件 kube-apiserver kube-proxy kube-controller-manager 等

    https://kubernetes.io/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container

    #添加了一个临时容器
    kubectl debug -ti nginx-deployment-656f474ddc-jhdqk -n ingress-nginx-test --image=busybox:1.28 --target=nginx
    #查看添加进去的容器
    kubectl describe pod nginx-deployment-656f474ddc-jhdqk -n ingress-nginx-test

    Containers:
      nginx:
        Container ID:   docker://2c3981fb9078894b6096ee4d51e01bbdf6ba30563a29ef9362e88f705292bfc3
        Image:          nginx:alpine
        Image ID:       docker-pullable://nginx@sha256:082f8c10bd47b6acc8ef15ae61ae45dd8fde0e9f389a8b5cb23c37408642bf5d
        Port:           80/TCP
        Host Port:      0/TCP
        State:          Running
          Started:      Tue, 06 Sep 2022 10:17:49 +0800
        Ready:          True
        Restart Count:  0
        Environment:    <none>
        Mounts:
          /usr/share/nginx/html from www (rw)
          /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hznzw (ro)
    Ephemeral Containers:
      debugger-86w8t:
        Container ID:   docker://620c4eff47bd9803db9e249728f70d7e864e92154ebb5572f045d9b1dd84c9c5
        Image:          busybox:1.28
        Image ID:       docker-pullable://busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47
        Port:           <none>
        Host Port:      <none>
        State:          Terminated
          Reason:       Completed
          Exit Code:    0
          Started:      Tue, 06 Sep 2022 17:42:09 +0800
          Finished:     Tue, 06 Sep 2022 17:48:17 +0800
        Ready:          False
        Restart Count:  0
        Environment:    <none>
        Mounts:         <none>
      debugger-stv76:
        Container ID:   docker://911547f4f6d20235c1d7754af05eff48d9aa6e683abe8dc5a6dacb43c127882e
        Image:          busybox:1.28
        Image ID:       docker-pullable://busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47
        Port:           <none>
        Host Port:      <none>
        State:          Running     ################### 正在运行中
          Started:      Tue, 06 Sep 2022 17:48:45 +0800
        Ready:          False
        Restart Count:  0
        Environment:    <none>
        Mounts:         <none>





## RBAC
    基于角色的访问控制。Role-Based Access Control。
    是一种基于企业内个人角色来管理一些资源的访问方法

    jenkins使用基于角色的用户权限管理。

    RBAC：4个顶级资源，Role, ClusterRole, RoleBinding, ClusterRoleBinding

    Role: 角色，包含一组权限的规则。没有拒绝规则，只有附加允许。Namespace隔离，只作用于名称空间内。
    ClusterRole 和 Role 的区别，Role只能作用在 Namespace中。ClusterRole针对整个集群。

    Rolebinding 将 ServiceAccount 和 Role 绑定在一起、














