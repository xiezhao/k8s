疑问:
  1. 不同主机，需要监控的数据不同，alertmanager怎么知道要去哪个主机里面监控主机的rule？
    Prometheus监控中，使用的是 PromSQL，比如 net_response_result_code，查询的是Prometheus监控的所有的主机有这个指标的，会去自己的TSDB中查找
    所以所有这个指标不满足alertmanager的alert的话，则alertmanager都会去通知，而不管是什么主机
    所以alertmanager中关心的是指标，而不需要关心什么服务跑在什么环境或者什么机器中，只要满足这个alert就报警
    因为Prometheus会把所有的主机暴露这个metrics的数据查出来


1. prometheus安装

# /lib/systemd/system/prometheus.service
[Unit]
Description=Prometheus Service
After=network.target

[Service]
Type=simple
User=root
Restart=on-failure
RestartSec=5s
ExecStart=/usr/local/prometheus/prometheus \
        --config.file=/usr/local/prometheus/prometheus.yml \
        --storage.tsdb.path=/usr/local/prometheus/data \
        --storage.tsdb.retention=15d \
        --log.format=json \
        --web.enable-lifecycle
[Install]
WantedBy=multi-user.target


systemctl enable prometheus
systemctl start prometheus



2. grafana
    2.1 直接docker安装就可以
    2.2 可以rpm安装
        https://grafana.com/grafana/download
        wget https://dl.grafana.com/enterprise/release/grafana-enterprise-8.3.6-1.x86_64.rpm
        sudo yum install grafana-enterprise-8.3.6-1.x86_64.rpm
        systemctl start grafana-server.service
        http://ip:3000/login


3. alertmanager安装

# /lib/systemd/system/alertmanager.service

[Unit]
Description=alertmanager

[Service]
WorkingDirectory=/usr/local/alertmanager/
ExecStart=/usr/local/alertmanager/alertmanager --config.file=/usr/local/alertmanager/alertmanager.yml --storage.path=/usr/local/alertmanager/data --web.listen-address=:9093 --data.retention=120h
Restart=on-failure

[Install]
WantedBy=multi-user.target



systemctl enable alertmanager
systemctl start alertmanager


检查配置文件：
  ./amtool check-config alertmanager.yml
  ./promtool check config prometheus.yml
重新加载配置文件：
  curl -X POST http://localhost:9090/-/reload




global:
    全局配置，主要配置告警方式，如邮件，webhook等
route：
    Prometheus告警先到达alertmanager都根路由（不能包含任何匹配项），所有告警的入口点
    根路由配置一个接收器(receiver)，处理那些没有匹配到任何子路由的告警（没有配置子路由，全部由根路由发送告警）
group_by:
    用于分组聚合，对告警通知按标签(label)进行分组，将具有相同标签或相同告警名称(alertname)的告警通知聚合在一个组，
    然后作为一个通知发送。如果想完全禁用聚合，可以设置为group_by:[...]
group_wait:
    当一个新的告警组被创建时，需要等待'group_wait'后才发送初始通知。这样可以确保在发送等待前能收集更多具有相同标签的告警，最后合并为一个通知发送。
group_interval:
    当第一次告警通知发出后，等待'group_interval'时间后，开始发送为该组触发的新告警，可以理解为，group相当于一个通道(channel)
repeat_interval:
    告警通知成功发送后，若问题一直未恢复，需再次重复发送的间隔





node_rules.yml

groups:
  - name: node_rules
    rules:
    - record: instance:node_cpu_usage
      expr: 100 - avg(irate(node_cpu_seconds_total){mode="idle"}[1m])) by (nodename) * 100
      labels:
        metric_type: cpu_monitor

    - record: instance: node_1m_load
      expr: node_load1
      labels:
        metric_type: load1m_monitor

    - record: instance: node_mem_usage
      expr: 100 - (node_memory_MemAvailable_bytes)/(node_memory_MemTotal_bytes) * 100
      labels:
        metric_type: Memory_monitor

    - record: instance: node_root_partition_monitor
      expr: round(predict_linear(node_filesystem_free_bytes{device="rootfs", mountpoint="/"}[2h],12*3600)/(1024*1024*1024), 2)
      labels:
        metric_type: root_partition_monitor


node_alerts.yml


[root@VM-4-3-centos alerts]# cat springboot_alerts.yml
groups:
  - name: springboot_alerts
    rules:
    - alert: springboot_health_check
      expr: http_response_result_code != 0
      for: 10s   #持续时间
      labels:
        severity: warning
      annotations:
        #发出的告警标题
        summary: "实例 {{ $labels.instance }} health check failure."
        #发出的告警内容
        description: "实例{{ $labels.instance }} health check failure."







groups:
  - name: node_alerts
    rules:
    - alert: cpu_usage_over_threshold
      expr: instance:node_cpu_usage > 80
      for: 1m   #持续时间
      labels:
        severity: warning
      annotations:
        #发出的告警标题
        summary: "实例 {{ $labels.instance }} CPU 使用率过高"
        #发出的告警内容
        description: "实例{{ $labels.instance }} CPU 使用率超过 85% (当前值为: {{ $value }})"

    - alert: system_1m_load_over_threshold
      expr: instance:node_1m_load > 20
      for: 1m
      labels:
        severity: warning
      annotations:
        summary: 主机{{ $labels.nodename }} 的1分负载超出阈值20%，当前为 {{$value}} %

    - alert: mem_usage_over_threshold
      expr: instance:node_mem_usage > 80
      for: 1m
      annotations:
        summary: 主机{{ $labels.nodename }} 的内存使用率持续1分钟超出阈值，当前为 {{$value}} %

    - alert: root_partition_usage_over_threshold
      expr: instance:node_root_partition_predit < 60
      for: 1m
      annotations:
        summary: 主机{{ $labels.nodename }} 的磁盘根分区预计在12小时使用将达到 {{$value}}GB，请及时扩容 %




alertmanager.yaml 示例：

global:
  resolve_timeout: 5m
  smtp_smarthost: 'smtp.qq.com:465'
  smtp_from: '145xxx8387@qq.com'
  smtp_auth_username: '1451578387@qq.com'
  smtp_auth_identity: 'xxxxx'
  smtp_auth_password: 'xxxxx'
  smtp_require_tls: false

# 根路由，不能存在 match和match_re，任何告警数据没有匹配到路由时，将会由此根路由进行处理。
route:
  group_by: ['job']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 120s
  receiver: 'default-receiver'
  routes:
  - match_re:
      alertname: 'Cpu.*'  # 如果告警的名字是以 Cpu 开头的发给 receiver-01
    receiver: 'receiver-01'
  - match:
      alertname: 'InstanceDown' # 如果告警的名字是 InstanceDown 则发送给 receiver-02
    receiver: 'receiver-02'
    group_by: ['instance'] # 根据 instance 标签分组
    continue: true  # 为true则还需要去匹配子路由。
    routes:
    - match:
        alertname: 'InstanceDown' # 如果告警的名字是 InstanceDown 则还是需要发送给 receiver-03
      receiver: 'receiver-03'

# 定义4个接收人（接收组等等）
receivers:
  - name: 'default-receiver'
    email_configs:
      - to: '145xxx8387@qq.com'
        send_resolved: true
  - name: 'receiver-01'
    email_configs:
      - to: '2469xxx193@qq.com'
        send_resolved: true
  - name: 'receiver-02'
    email_configs:
      - to: 'weixin145xxx8387@163.com'
        send_resolved: true
  - name: 'receiver-03'
    email_configs:
      - to: 'it_xxx_software@163.com'
        send_resolved: true

inhibit_rules:
  - source_match:
      alertname: Cpu02
      severity: warning
    target_match:
      severity: info
    equal:
      - instance







{
  "version": "4",
  "groupKey": <string>,              // key identifying the group of alerts (e.g. to deduplicate)
  "truncatedAlerts": <int>,          // how many alerts have been truncated due to "max_alerts"
  "status": "<resolved|firing>",
  "receiver": <string>,
  "groupLabels": <object>,
  "commonLabels": <object>,
  "commonAnnotations": <object>,
  "externalURL": <string>,           // backlink to the Alertmanager.
  "alerts": [
    {
      "status": "<resolved|firing>",
      "labels": <object>,
      "annotations": <object>,
      "startsAt": "<rfc3339>",
      "endsAt": "<rfc3339>",
      "generatorURL": <string>,      // identifies the entity that caused the alert
      "fingerprint": <string>        // fingerprint to identify the alert
    },
    ...
  ]
}